{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import dill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator GridSearchCV from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "with open('gridsearch_full_training_set.dill', 'rb') as f:\n",
    "    gs = dill.load(f)\n",
    "    dill.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(gs.cv_results_)\n",
    "\n",
    "with open('latex_table_gridsearch.txt', 'w') as f:\n",
    "    df.to_latex(buf=f,columns=['mean_test_score', 'param_batch_size', 'param_lr',\n",
    "       'param_module__n_hidden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Volumes/Macintosh_HD/Users/Aron/miniconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_lr</th>\n",
       "      <th>param_module__n_hidden</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>274.826207</td>\n",
       "      <td>3.349877</td>\n",
       "      <td>0.11242</td>\n",
       "      <td>0.113450</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 64, 'lr': 0.1, 'module__n_hidde...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.100250</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>0.100575</td>\n",
       "      <td>21.123144</td>\n",
       "      <td>0.141324</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>0.026551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>343.419399</td>\n",
       "      <td>4.645695</td>\n",
       "      <td>0.11162</td>\n",
       "      <td>0.110325</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 64, 'lr': 0.1, 'module__n_hidde...</td>\n",
       "      <td>36</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>0.099625</td>\n",
       "      <td>0.1574</td>\n",
       "      <td>0.152750</td>\n",
       "      <td>0.0980</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>14.656351</td>\n",
       "      <td>0.799479</td>\n",
       "      <td>0.022979</td>\n",
       "      <td>0.021217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167.578540</td>\n",
       "      <td>2.946457</td>\n",
       "      <td>0.11866</td>\n",
       "      <td>0.118715</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 64, 'lr': 0.1, 'module__n_hidde...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.1377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.080675</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.099475</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.146050</td>\n",
       "      <td>6.034886</td>\n",
       "      <td>0.255656</td>\n",
       "      <td>0.023935</td>\n",
       "      <td>0.024661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>291.341127</td>\n",
       "      <td>3.419174</td>\n",
       "      <td>0.37482</td>\n",
       "      <td>0.393390</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 64, 'lr': 0.01, 'module__n_hidd...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3995</td>\n",
       "      <td>0.410950</td>\n",
       "      <td>0.3919</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.3294</td>\n",
       "      <td>0.351075</td>\n",
       "      <td>1.822038</td>\n",
       "      <td>0.234182</td>\n",
       "      <td>0.028401</td>\n",
       "      <td>0.025922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>440.829736</td>\n",
       "      <td>4.764672</td>\n",
       "      <td>0.25468</td>\n",
       "      <td>0.260135</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 64, 'lr': 0.01, 'module__n_hidd...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.2692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2612</td>\n",
       "      <td>0.261250</td>\n",
       "      <td>0.2388</td>\n",
       "      <td>0.244900</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.259775</td>\n",
       "      <td>15.495570</td>\n",
       "      <td>0.457306</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.008871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>177.778569</td>\n",
       "      <td>2.865364</td>\n",
       "      <td>0.36146</td>\n",
       "      <td>0.401400</td>\n",
       "      <td>64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 64, 'lr': 0.01, 'module__n_hidd...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.4022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.452475</td>\n",
       "      <td>0.2975</td>\n",
       "      <td>0.336550</td>\n",
       "      <td>0.3160</td>\n",
       "      <td>0.360075</td>\n",
       "      <td>4.590429</td>\n",
       "      <td>0.219847</td>\n",
       "      <td>0.046751</td>\n",
       "      <td>0.044958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>278.618461</td>\n",
       "      <td>3.152317</td>\n",
       "      <td>0.37192</td>\n",
       "      <td>0.431030</td>\n",
       "      <td>64</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 64, 'lr': 0.001, 'module__n_hid...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.400550</td>\n",
       "      <td>0.3859</td>\n",
       "      <td>0.450975</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.429475</td>\n",
       "      <td>14.144882</td>\n",
       "      <td>0.295050</td>\n",
       "      <td>0.012993</td>\n",
       "      <td>0.018439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>349.099040</td>\n",
       "      <td>4.315320</td>\n",
       "      <td>0.39170</td>\n",
       "      <td>0.433175</td>\n",
       "      <td>64</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 64, 'lr': 0.001, 'module__n_hid...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3856</td>\n",
       "      <td>0.410350</td>\n",
       "      <td>0.3640</td>\n",
       "      <td>0.407850</td>\n",
       "      <td>0.4164</td>\n",
       "      <td>0.466725</td>\n",
       "      <td>18.853490</td>\n",
       "      <td>0.422021</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.024164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>171.923416</td>\n",
       "      <td>3.104642</td>\n",
       "      <td>0.37270</td>\n",
       "      <td>0.411470</td>\n",
       "      <td>64</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 64, 'lr': 0.001, 'module__n_hid...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3809</td>\n",
       "      <td>0.413650</td>\n",
       "      <td>0.3649</td>\n",
       "      <td>0.399050</td>\n",
       "      <td>0.3567</td>\n",
       "      <td>0.401050</td>\n",
       "      <td>8.486410</td>\n",
       "      <td>0.373640</td>\n",
       "      <td>0.010129</td>\n",
       "      <td>0.011238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162.222411</td>\n",
       "      <td>2.316466</td>\n",
       "      <td>0.15542</td>\n",
       "      <td>0.157715</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 128, 'lr': 0.1, 'module__n_hidd...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.1274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>0.180100</td>\n",
       "      <td>0.1707</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.165200</td>\n",
       "      <td>2.515305</td>\n",
       "      <td>0.123780</td>\n",
       "      <td>0.018088</td>\n",
       "      <td>0.017657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>235.544663</td>\n",
       "      <td>3.108820</td>\n",
       "      <td>0.14622</td>\n",
       "      <td>0.147790</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 128, 'lr': 0.1, 'module__n_hidd...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.1001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1663</td>\n",
       "      <td>0.173875</td>\n",
       "      <td>0.1789</td>\n",
       "      <td>0.177425</td>\n",
       "      <td>0.1623</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>7.737392</td>\n",
       "      <td>0.177974</td>\n",
       "      <td>0.029574</td>\n",
       "      <td>0.030291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>109.419207</td>\n",
       "      <td>1.836844</td>\n",
       "      <td>0.15166</td>\n",
       "      <td>0.153490</td>\n",
       "      <td>128</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 128, 'lr': 0.1, 'module__n_hidd...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>0.134375</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.164800</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>0.183950</td>\n",
       "      <td>2.532476</td>\n",
       "      <td>0.272804</td>\n",
       "      <td>0.026180</td>\n",
       "      <td>0.025309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>163.563123</td>\n",
       "      <td>2.449166</td>\n",
       "      <td>0.41258</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>128</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 128, 'lr': 0.01, 'module__n_hid...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4399</td>\n",
       "      <td>0.484600</td>\n",
       "      <td>0.3870</td>\n",
       "      <td>0.441750</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.454875</td>\n",
       "      <td>12.982639</td>\n",
       "      <td>0.283064</td>\n",
       "      <td>0.017942</td>\n",
       "      <td>0.014561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>345.155967</td>\n",
       "      <td>3.362940</td>\n",
       "      <td>0.32458</td>\n",
       "      <td>0.336985</td>\n",
       "      <td>128</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 128, 'lr': 0.01, 'module__n_hid...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.2963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3191</td>\n",
       "      <td>0.321350</td>\n",
       "      <td>0.3365</td>\n",
       "      <td>0.356650</td>\n",
       "      <td>0.3465</td>\n",
       "      <td>0.364150</td>\n",
       "      <td>1.480518</td>\n",
       "      <td>0.268540</td>\n",
       "      <td>0.017043</td>\n",
       "      <td>0.022938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>115.918270</td>\n",
       "      <td>2.113665</td>\n",
       "      <td>0.38232</td>\n",
       "      <td>0.430870</td>\n",
       "      <td>128</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 128, 'lr': 0.01, 'module__n_hid...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.3907</td>\n",
       "      <td>0.442450</td>\n",
       "      <td>0.3786</td>\n",
       "      <td>0.428225</td>\n",
       "      <td>2.256344</td>\n",
       "      <td>0.240225</td>\n",
       "      <td>0.026917</td>\n",
       "      <td>0.032437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>159.318499</td>\n",
       "      <td>2.143808</td>\n",
       "      <td>0.37902</td>\n",
       "      <td>0.435735</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 128, 'lr': 0.001, 'module__n_hi...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3713</td>\n",
       "      <td>0.420225</td>\n",
       "      <td>0.3838</td>\n",
       "      <td>0.437550</td>\n",
       "      <td>0.3745</td>\n",
       "      <td>0.436250</td>\n",
       "      <td>3.221163</td>\n",
       "      <td>0.146723</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>0.008824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>227.608456</td>\n",
       "      <td>3.079098</td>\n",
       "      <td>0.35014</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 128, 'lr': 0.001, 'module__n_hi...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.3239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4001</td>\n",
       "      <td>0.438025</td>\n",
       "      <td>0.3554</td>\n",
       "      <td>0.409125</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.352475</td>\n",
       "      <td>6.638393</td>\n",
       "      <td>0.402626</td>\n",
       "      <td>0.031458</td>\n",
       "      <td>0.032388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>105.522072</td>\n",
       "      <td>1.878584</td>\n",
       "      <td>0.33658</td>\n",
       "      <td>0.359995</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 128, 'lr': 0.001, 'module__n_hi...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3209</td>\n",
       "      <td>0.330825</td>\n",
       "      <td>0.3338</td>\n",
       "      <td>0.357075</td>\n",
       "      <td>0.3425</td>\n",
       "      <td>0.371125</td>\n",
       "      <td>3.971786</td>\n",
       "      <td>0.282827</td>\n",
       "      <td>0.011768</td>\n",
       "      <td>0.021178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>110.151693</td>\n",
       "      <td>1.560335</td>\n",
       "      <td>0.22294</td>\n",
       "      <td>0.224325</td>\n",
       "      <td>256</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 256, 'lr': 0.1, 'module__n_hidd...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2483</td>\n",
       "      <td>0.244125</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.209775</td>\n",
       "      <td>0.2267</td>\n",
       "      <td>0.235825</td>\n",
       "      <td>5.382476</td>\n",
       "      <td>0.097263</td>\n",
       "      <td>0.014614</td>\n",
       "      <td>0.013325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>169.572960</td>\n",
       "      <td>1.894762</td>\n",
       "      <td>0.19582</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>256</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 256, 'lr': 0.1, 'module__n_hidd...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.194250</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.1999</td>\n",
       "      <td>0.204375</td>\n",
       "      <td>4.519651</td>\n",
       "      <td>0.065391</td>\n",
       "      <td>0.013036</td>\n",
       "      <td>0.014649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>79.677266</td>\n",
       "      <td>1.454267</td>\n",
       "      <td>0.32544</td>\n",
       "      <td>0.334780</td>\n",
       "      <td>256</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 256, 'lr': 0.1, 'module__n_hidd...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3537</td>\n",
       "      <td>0.357750</td>\n",
       "      <td>0.3236</td>\n",
       "      <td>0.333575</td>\n",
       "      <td>0.3464</td>\n",
       "      <td>0.360650</td>\n",
       "      <td>1.265125</td>\n",
       "      <td>0.049635</td>\n",
       "      <td>0.022183</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100.142279</td>\n",
       "      <td>1.551269</td>\n",
       "      <td>0.41644</td>\n",
       "      <td>0.484380</td>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 256, 'lr': 0.01, 'module__n_hid...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4362</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>0.3861</td>\n",
       "      <td>0.467375</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>7.388684</td>\n",
       "      <td>0.153890</td>\n",
       "      <td>0.021906</td>\n",
       "      <td>0.018832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>212.838757</td>\n",
       "      <td>2.028607</td>\n",
       "      <td>0.35870</td>\n",
       "      <td>0.381910</td>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 256, 'lr': 0.01, 'module__n_hid...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.3770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3984</td>\n",
       "      <td>0.413750</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.318875</td>\n",
       "      <td>0.3152</td>\n",
       "      <td>0.335225</td>\n",
       "      <td>6.404882</td>\n",
       "      <td>0.089295</td>\n",
       "      <td>0.041201</td>\n",
       "      <td>0.047037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>80.962047</td>\n",
       "      <td>1.479586</td>\n",
       "      <td>0.38614</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 256, 'lr': 0.01, 'module__n_hid...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3955</td>\n",
       "      <td>0.439000</td>\n",
       "      <td>0.3970</td>\n",
       "      <td>0.445150</td>\n",
       "      <td>0.3851</td>\n",
       "      <td>0.441225</td>\n",
       "      <td>2.462022</td>\n",
       "      <td>0.078239</td>\n",
       "      <td>0.009220</td>\n",
       "      <td>0.007224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>95.458588</td>\n",
       "      <td>1.423786</td>\n",
       "      <td>0.32568</td>\n",
       "      <td>0.361240</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 256, 'lr': 0.001, 'module__n_hi...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3552</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.3081</td>\n",
       "      <td>0.345350</td>\n",
       "      <td>0.3315</td>\n",
       "      <td>0.383875</td>\n",
       "      <td>0.798395</td>\n",
       "      <td>0.052581</td>\n",
       "      <td>0.016567</td>\n",
       "      <td>0.020540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>137.728850</td>\n",
       "      <td>1.816705</td>\n",
       "      <td>0.35258</td>\n",
       "      <td>0.394760</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 256, 'lr': 0.001, 'module__n_hi...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3834</td>\n",
       "      <td>0.416650</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.433625</td>\n",
       "      <td>0.3207</td>\n",
       "      <td>0.358375</td>\n",
       "      <td>0.780320</td>\n",
       "      <td>0.088108</td>\n",
       "      <td>0.026092</td>\n",
       "      <td>0.027610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>72.265980</td>\n",
       "      <td>1.232162</td>\n",
       "      <td>0.29142</td>\n",
       "      <td>0.302995</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 256, 'lr': 0.001, 'module__n_hi...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2702</td>\n",
       "      <td>0.278450</td>\n",
       "      <td>0.2925</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.303675</td>\n",
       "      <td>1.448890</td>\n",
       "      <td>0.092440</td>\n",
       "      <td>0.024352</td>\n",
       "      <td>0.023872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>60.765281</td>\n",
       "      <td>0.742545</td>\n",
       "      <td>0.32258</td>\n",
       "      <td>0.333450</td>\n",
       "      <td>512</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 512, 'lr': 0.1, 'module__n_hidd...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>0.2672</td>\n",
       "      <td>0.274025</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>11.545559</td>\n",
       "      <td>0.135257</td>\n",
       "      <td>0.030927</td>\n",
       "      <td>0.032410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>166.376794</td>\n",
       "      <td>1.415462</td>\n",
       "      <td>0.23844</td>\n",
       "      <td>0.238410</td>\n",
       "      <td>512</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 512, 'lr': 0.1, 'module__n_hidd...</td>\n",
       "      <td>28</td>\n",
       "      <td>0.2278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2785</td>\n",
       "      <td>0.279525</td>\n",
       "      <td>0.2661</td>\n",
       "      <td>0.268150</td>\n",
       "      <td>0.2452</td>\n",
       "      <td>0.244150</td>\n",
       "      <td>22.733515</td>\n",
       "      <td>0.272039</td>\n",
       "      <td>0.036344</td>\n",
       "      <td>0.035379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>47.523040</td>\n",
       "      <td>0.644710</td>\n",
       "      <td>0.38744</td>\n",
       "      <td>0.418670</td>\n",
       "      <td>512</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 512, 'lr': 0.1, 'module__n_hidd...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4003</td>\n",
       "      <td>0.422450</td>\n",
       "      <td>0.4054</td>\n",
       "      <td>0.445350</td>\n",
       "      <td>0.3710</td>\n",
       "      <td>0.408850</td>\n",
       "      <td>5.222192</td>\n",
       "      <td>0.049841</td>\n",
       "      <td>0.023150</td>\n",
       "      <td>0.021947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>48.738305</td>\n",
       "      <td>0.665327</td>\n",
       "      <td>0.40430</td>\n",
       "      <td>0.474625</td>\n",
       "      <td>512</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 512, 'lr': 0.01, 'module__n_hid...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4119</td>\n",
       "      <td>0.480575</td>\n",
       "      <td>0.3867</td>\n",
       "      <td>0.457025</td>\n",
       "      <td>0.3975</td>\n",
       "      <td>0.459675</td>\n",
       "      <td>6.962166</td>\n",
       "      <td>0.035968</td>\n",
       "      <td>0.011827</td>\n",
       "      <td>0.013710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>122.177559</td>\n",
       "      <td>1.327460</td>\n",
       "      <td>0.36790</td>\n",
       "      <td>0.401235</td>\n",
       "      <td>512</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 512, 'lr': 0.01, 'module__n_hid...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4086</td>\n",
       "      <td>0.435025</td>\n",
       "      <td>0.3427</td>\n",
       "      <td>0.377150</td>\n",
       "      <td>0.3601</td>\n",
       "      <td>0.398525</td>\n",
       "      <td>9.838608</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>0.027195</td>\n",
       "      <td>0.027883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>44.363938</td>\n",
       "      <td>0.577528</td>\n",
       "      <td>0.36450</td>\n",
       "      <td>0.399275</td>\n",
       "      <td>512</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 512, 'lr': 0.01, 'module__n_hid...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.3608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3798</td>\n",
       "      <td>0.409425</td>\n",
       "      <td>0.3459</td>\n",
       "      <td>0.389700</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>0.381475</td>\n",
       "      <td>3.905605</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>0.016559</td>\n",
       "      <td>0.016414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>44.806123</td>\n",
       "      <td>0.644764</td>\n",
       "      <td>0.31560</td>\n",
       "      <td>0.339125</td>\n",
       "      <td>512</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'batch_size': 512, 'lr': 0.001, 'module__n_hi...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.3159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3183</td>\n",
       "      <td>0.336025</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.335625</td>\n",
       "      <td>0.3133</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>5.087124</td>\n",
       "      <td>0.083756</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.003494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>96.421369</td>\n",
       "      <td>1.258499</td>\n",
       "      <td>0.31206</td>\n",
       "      <td>0.343875</td>\n",
       "      <td>512</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[1000]</td>\n",
       "      <td>{'batch_size': 512, 'lr': 0.001, 'module__n_hi...</td>\n",
       "      <td>24</td>\n",
       "      <td>0.3419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3490</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>0.2923</td>\n",
       "      <td>0.323425</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.321025</td>\n",
       "      <td>6.863277</td>\n",
       "      <td>0.116193</td>\n",
       "      <td>0.027405</td>\n",
       "      <td>0.026756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>40.127870</td>\n",
       "      <td>0.506368</td>\n",
       "      <td>0.26624</td>\n",
       "      <td>0.273275</td>\n",
       "      <td>512</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[50, 30, 20]</td>\n",
       "      <td>{'batch_size': 512, 'lr': 0.001, 'module__n_hi...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.2853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2847</td>\n",
       "      <td>0.277975</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>0.241450</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>0.280325</td>\n",
       "      <td>3.538366</td>\n",
       "      <td>0.126453</td>\n",
       "      <td>0.021262</td>\n",
       "      <td>0.018210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      274.826207         3.349877          0.11242          0.113450   \n",
       "1      343.419399         4.645695          0.11162          0.110325   \n",
       "2      167.578540         2.946457          0.11866          0.118715   \n",
       "3      291.341127         3.419174          0.37482          0.393390   \n",
       "4      440.829736         4.764672          0.25468          0.260135   \n",
       "5      177.778569         2.865364          0.36146          0.401400   \n",
       "6      278.618461         3.152317          0.37192          0.431030   \n",
       "7      349.099040         4.315320          0.39170          0.433175   \n",
       "8      171.923416         3.104642          0.37270          0.411470   \n",
       "9      162.222411         2.316466          0.15542          0.157715   \n",
       "10     235.544663         3.108820          0.14622          0.147790   \n",
       "11     109.419207         1.836844          0.15166          0.153490   \n",
       "12     163.563123         2.449166          0.41258          0.463800   \n",
       "13     345.155967         3.362940          0.32458          0.336985   \n",
       "14     115.918270         2.113665          0.38232          0.430870   \n",
       "15     159.318499         2.143808          0.37902          0.435735   \n",
       "16     227.608456         3.079098          0.35014          0.394700   \n",
       "17     105.522072         1.878584          0.33658          0.359995   \n",
       "18     110.151693         1.560335          0.22294          0.224325   \n",
       "19     169.572960         1.894762          0.19582          0.194300   \n",
       "20      79.677266         1.454267          0.32544          0.334780   \n",
       "21     100.142279         1.551269          0.41644          0.484380   \n",
       "22     212.838757         2.028607          0.35870          0.381910   \n",
       "23      80.962047         1.479586          0.38614          0.436270   \n",
       "24      95.458588         1.423786          0.32568          0.361240   \n",
       "25     137.728850         1.816705          0.35258          0.394760   \n",
       "26      72.265980         1.232162          0.29142          0.302995   \n",
       "27      60.765281         0.742545          0.32258          0.333450   \n",
       "28     166.376794         1.415462          0.23844          0.238410   \n",
       "29      47.523040         0.644710          0.38744          0.418670   \n",
       "30      48.738305         0.665327          0.40430          0.474625   \n",
       "31     122.177559         1.327460          0.36790          0.401235   \n",
       "32      44.363938         0.577528          0.36450          0.399275   \n",
       "33      44.806123         0.644764          0.31560          0.339125   \n",
       "34      96.421369         1.258499          0.31206          0.343875   \n",
       "35      40.127870         0.506368          0.26624          0.273275   \n",
       "\n",
       "   param_batch_size param_lr param_module__n_hidden  \\\n",
       "0                64      0.1             [100, 100]   \n",
       "1                64      0.1                 [1000]   \n",
       "2                64      0.1           [50, 30, 20]   \n",
       "3                64     0.01             [100, 100]   \n",
       "4                64     0.01                 [1000]   \n",
       "5                64     0.01           [50, 30, 20]   \n",
       "6                64    0.001             [100, 100]   \n",
       "7                64    0.001                 [1000]   \n",
       "8                64    0.001           [50, 30, 20]   \n",
       "9               128      0.1             [100, 100]   \n",
       "10              128      0.1                 [1000]   \n",
       "11              128      0.1           [50, 30, 20]   \n",
       "12              128     0.01             [100, 100]   \n",
       "13              128     0.01                 [1000]   \n",
       "14              128     0.01           [50, 30, 20]   \n",
       "15              128    0.001             [100, 100]   \n",
       "16              128    0.001                 [1000]   \n",
       "17              128    0.001           [50, 30, 20]   \n",
       "18              256      0.1             [100, 100]   \n",
       "19              256      0.1                 [1000]   \n",
       "20              256      0.1           [50, 30, 20]   \n",
       "21              256     0.01             [100, 100]   \n",
       "22              256     0.01                 [1000]   \n",
       "23              256     0.01           [50, 30, 20]   \n",
       "24              256    0.001             [100, 100]   \n",
       "25              256    0.001                 [1000]   \n",
       "26              256    0.001           [50, 30, 20]   \n",
       "27              512      0.1             [100, 100]   \n",
       "28              512      0.1                 [1000]   \n",
       "29              512      0.1           [50, 30, 20]   \n",
       "30              512     0.01             [100, 100]   \n",
       "31              512     0.01                 [1000]   \n",
       "32              512     0.01           [50, 30, 20]   \n",
       "33              512    0.001             [100, 100]   \n",
       "34              512    0.001                 [1000]   \n",
       "35              512    0.001           [50, 30, 20]   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "0   {'batch_size': 64, 'lr': 0.1, 'module__n_hidde...               35   \n",
       "1   {'batch_size': 64, 'lr': 0.1, 'module__n_hidde...               36   \n",
       "2   {'batch_size': 64, 'lr': 0.1, 'module__n_hidde...               34   \n",
       "3   {'batch_size': 64, 'lr': 0.01, 'module__n_hidd...                9   \n",
       "4   {'batch_size': 64, 'lr': 0.01, 'module__n_hidd...               27   \n",
       "5   {'batch_size': 64, 'lr': 0.01, 'module__n_hidd...               14   \n",
       "6   {'batch_size': 64, 'lr': 0.001, 'module__n_hid...               11   \n",
       "7   {'batch_size': 64, 'lr': 0.001, 'module__n_hid...                4   \n",
       "8   {'batch_size': 64, 'lr': 0.001, 'module__n_hid...               10   \n",
       "9   {'batch_size': 128, 'lr': 0.1, 'module__n_hidd...               31   \n",
       "10  {'batch_size': 128, 'lr': 0.1, 'module__n_hidd...               33   \n",
       "11  {'batch_size': 128, 'lr': 0.1, 'module__n_hidd...               32   \n",
       "12  {'batch_size': 128, 'lr': 0.01, 'module__n_hid...                2   \n",
       "13  {'batch_size': 128, 'lr': 0.01, 'module__n_hid...               21   \n",
       "14  {'batch_size': 128, 'lr': 0.01, 'module__n_hid...                7   \n",
       "15  {'batch_size': 128, 'lr': 0.001, 'module__n_hi...                8   \n",
       "16  {'batch_size': 128, 'lr': 0.001, 'module__n_hi...               17   \n",
       "17  {'batch_size': 128, 'lr': 0.001, 'module__n_hi...               18   \n",
       "18  {'batch_size': 256, 'lr': 0.1, 'module__n_hidd...               29   \n",
       "19  {'batch_size': 256, 'lr': 0.1, 'module__n_hidd...               30   \n",
       "20  {'batch_size': 256, 'lr': 0.1, 'module__n_hidd...               20   \n",
       "21  {'batch_size': 256, 'lr': 0.01, 'module__n_hid...                1   \n",
       "22  {'batch_size': 256, 'lr': 0.01, 'module__n_hid...               15   \n",
       "23  {'batch_size': 256, 'lr': 0.01, 'module__n_hid...                6   \n",
       "24  {'batch_size': 256, 'lr': 0.001, 'module__n_hi...               19   \n",
       "25  {'batch_size': 256, 'lr': 0.001, 'module__n_hi...               16   \n",
       "26  {'batch_size': 256, 'lr': 0.001, 'module__n_hi...               25   \n",
       "27  {'batch_size': 512, 'lr': 0.1, 'module__n_hidd...               22   \n",
       "28  {'batch_size': 512, 'lr': 0.1, 'module__n_hidd...               28   \n",
       "29  {'batch_size': 512, 'lr': 0.1, 'module__n_hidd...                5   \n",
       "30  {'batch_size': 512, 'lr': 0.01, 'module__n_hid...                3   \n",
       "31  {'batch_size': 512, 'lr': 0.01, 'module__n_hid...               12   \n",
       "32  {'batch_size': 512, 'lr': 0.01, 'module__n_hid...               13   \n",
       "33  {'batch_size': 512, 'lr': 0.001, 'module__n_hi...               23   \n",
       "34  {'batch_size': 512, 'lr': 0.001, 'module__n_hi...               24   \n",
       "35  {'batch_size': 512, 'lr': 0.001, 'module__n_hi...               26   \n",
       "\n",
       "    split0_test_score       ...         split2_test_score  split2_train_score  \\\n",
       "0              0.1001       ...                    0.0990            0.100250   \n",
       "1              0.1032       ...                    0.1015            0.099625   \n",
       "2              0.1377       ...                    0.0806            0.080675   \n",
       "3              0.3997       ...                    0.3995            0.410950   \n",
       "4              0.2692       ...                    0.2612            0.261250   \n",
       "5              0.4022       ...                    0.4152            0.452475   \n",
       "6              0.3894       ...                    0.3585            0.400550   \n",
       "7              0.4174       ...                    0.3856            0.410350   \n",
       "8              0.3786       ...                    0.3809            0.413650   \n",
       "9              0.1274       ...                    0.1735            0.180100   \n",
       "10             0.1001       ...                    0.1663            0.173875   \n",
       "11             0.1079       ...                    0.1390            0.134375   \n",
       "12             0.4229       ...                    0.4399            0.484600   \n",
       "13             0.2963       ...                    0.3191            0.321350   \n",
       "14             0.3338       ...                    0.3937            0.428200   \n",
       "15             0.3858       ...                    0.3713            0.420225   \n",
       "16             0.3239       ...                    0.4001            0.438025   \n",
       "17             0.3301       ...                    0.3209            0.330825   \n",
       "18             0.2224       ...                    0.2483            0.244125   \n",
       "19             0.1987       ...                    0.1929            0.194250   \n",
       "20             0.3087       ...                    0.3537            0.357750   \n",
       "21             0.4450       ...                    0.4362            0.497300   \n",
       "22             0.3770       ...                    0.3984            0.413750   \n",
       "23             0.3807       ...                    0.3955            0.439000   \n",
       "24             0.3165       ...                    0.3552            0.388600   \n",
       "25             0.3522       ...                    0.3834            0.416650   \n",
       "26             0.3370       ...                    0.2702            0.278450   \n",
       "27             0.3613       ...                    0.3349            0.341600   \n",
       "28             0.2278       ...                    0.2785            0.279525   \n",
       "29             0.4104       ...                    0.4003            0.422450   \n",
       "30             0.4212       ...                    0.4119            0.480575   \n",
       "31             0.3384       ...                    0.4086            0.435025   \n",
       "32             0.3608       ...                    0.3798            0.409425   \n",
       "33             0.3159       ...                    0.3183            0.336025   \n",
       "34             0.3419       ...                    0.3490            0.378000   \n",
       "35             0.2853       ...                    0.2847            0.277975   \n",
       "\n",
       "    split3_test_score  split3_train_score  split4_test_score  \\\n",
       "0              0.1004            0.099900             0.0977   \n",
       "1              0.1574            0.152750             0.0980   \n",
       "2              0.1021            0.099475             0.1450   \n",
       "3              0.3919            0.411175             0.3294   \n",
       "4              0.2388            0.244900             0.2500   \n",
       "5              0.2975            0.336550             0.3160   \n",
       "6              0.3859            0.450975             0.3625   \n",
       "7              0.3640            0.407850             0.4164   \n",
       "8              0.3649            0.399050             0.3567   \n",
       "9              0.1707            0.169000             0.1645   \n",
       "10             0.1789            0.177425             0.1623   \n",
       "11             0.1601            0.164800             0.1837   \n",
       "12             0.3870            0.441750             0.4027   \n",
       "13             0.3365            0.356650             0.3465   \n",
       "14             0.3907            0.442450             0.3786   \n",
       "15             0.3838            0.437550             0.3745   \n",
       "16             0.3554            0.409125             0.3100   \n",
       "17             0.3338            0.357075             0.3425   \n",
       "18             0.2077            0.209775             0.2267   \n",
       "19             0.1737            0.170000             0.1999   \n",
       "20             0.3236            0.333575             0.3464   \n",
       "21             0.3861            0.467375             0.4000   \n",
       "22             0.3036            0.318875             0.3152   \n",
       "23             0.3970            0.445150             0.3851   \n",
       "24             0.3081            0.345350             0.3315   \n",
       "25             0.3801            0.433625             0.3207   \n",
       "26             0.2925            0.302900             0.2862   \n",
       "27             0.2672            0.274025             0.3300   \n",
       "28             0.2661            0.268150             0.2452   \n",
       "29             0.4054            0.445350             0.3710   \n",
       "30             0.3867            0.457025             0.3975   \n",
       "31             0.3427            0.377150             0.3601   \n",
       "32             0.3459            0.389700             0.3486   \n",
       "33             0.3196            0.335625             0.3133   \n",
       "34             0.2923            0.323425             0.2871   \n",
       "35             0.2309            0.241450             0.2775   \n",
       "\n",
       "    split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0             0.100575     21.123144        0.141324        0.026257   \n",
       "1             0.100500     14.656351        0.799479        0.022979   \n",
       "2             0.146050      6.034886        0.255656        0.023935   \n",
       "3             0.351075      1.822038        0.234182        0.028401   \n",
       "4             0.259775     15.495570        0.457306        0.010273   \n",
       "5             0.360075      4.590429        0.219847        0.046751   \n",
       "6             0.429475     14.144882        0.295050        0.012993   \n",
       "7             0.466725     18.853490        0.422021        0.021682   \n",
       "8             0.401050      8.486410        0.373640        0.010129   \n",
       "9             0.165200      2.515305        0.123780        0.018088   \n",
       "10            0.162700      7.737392        0.177974        0.029574   \n",
       "11            0.183950      2.532476        0.272804        0.026180   \n",
       "12            0.454875     12.982639        0.283064        0.017942   \n",
       "13            0.364150      1.480518        0.268540        0.017043   \n",
       "14            0.428225      2.256344        0.240225        0.026917   \n",
       "15            0.436250      3.221163        0.146723        0.005465   \n",
       "16            0.352475      6.638393        0.402626        0.031458   \n",
       "17            0.371125      3.971786        0.282827        0.011768   \n",
       "18            0.235825      5.382476        0.097263        0.014614   \n",
       "19            0.204375      4.519651        0.065391        0.013036   \n",
       "20            0.360650      1.265125        0.049635        0.022183   \n",
       "21            0.458300      7.388684        0.153890        0.021906   \n",
       "22            0.335225      6.404882        0.089295        0.041201   \n",
       "23            0.441225      2.462022        0.078239        0.009220   \n",
       "24            0.383875      0.798395        0.052581        0.016567   \n",
       "25            0.358375      0.780320        0.088108        0.026092   \n",
       "26            0.303675      1.448890        0.092440        0.024352   \n",
       "27            0.343700     11.545559        0.135257        0.030927   \n",
       "28            0.244150     22.733515        0.272039        0.036344   \n",
       "29            0.408850      5.222192        0.049841        0.023150   \n",
       "30            0.459675      6.962166        0.035968        0.011827   \n",
       "31            0.398525      9.838608        0.177028        0.027195   \n",
       "32            0.381475      3.905605        0.025820        0.016559   \n",
       "33            0.337800      5.087124        0.083756        0.003186   \n",
       "34            0.321025      6.863277        0.116193        0.027405   \n",
       "35            0.280325      3.538366        0.126453        0.021262   \n",
       "\n",
       "    std_train_score  \n",
       "0          0.026551  \n",
       "1          0.021217  \n",
       "2          0.024661  \n",
       "3          0.025922  \n",
       "4          0.008871  \n",
       "5          0.044958  \n",
       "6          0.018439  \n",
       "7          0.024164  \n",
       "8          0.011238  \n",
       "9          0.017657  \n",
       "10         0.030291  \n",
       "11         0.025309  \n",
       "12         0.014561  \n",
       "13         0.022938  \n",
       "14         0.032437  \n",
       "15         0.008824  \n",
       "16         0.032388  \n",
       "17         0.021178  \n",
       "18         0.013325  \n",
       "19         0.014649  \n",
       "20         0.021600  \n",
       "21         0.018832  \n",
       "22         0.047037  \n",
       "23         0.007224  \n",
       "24         0.020540  \n",
       "25         0.027610  \n",
       "26         0.023872  \n",
       "27         0.032410  \n",
       "28         0.035379  \n",
       "29         0.021947  \n",
       "30         0.013710  \n",
       "31         0.027883  \n",
       "32         0.016414  \n",
       "33         0.003494  \n",
       "34         0.026756  \n",
       "35         0.018210  \n",
       "\n",
       "[36 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8E2X+wPHPJD3TFii03Ee4M4CASIoHCHiwaET3cBVw13UVEc913V2N7uGx62/juofoioisB96uildcQZTDCyl4cUyAUsINpS1H7zbJ/P5IiqG0kLZJJ22/79crryQzzzP5jgdfnnkuRdd1hBBCiHhjMjoAIYQQoj6SoIQQQsQlSVBCCCHikiQoIYQQcUkSlBBCiLgkCUoIIURckgQlhBAiLkmCEkIIEZckQQkhhIhLCUYHEE0mk0lPTU01OgwhhDBUeXm5rut6q2+AtKkElZqaSllZmdFhCCGEoRRFqTA6hmhoUwlKCCFEZKxO91RgLmAGFnpdDlc9ZSYBjwCJQKHX5ZgYOu4FSgA/4PO6HGNjEWOrbwIKIYRoHKvTbQYeBy4ChgEzrE73sDplOgHzgEu9Lsdw4Kd1LjPZ63KMjlVyAklQQgjRHuUAeV6XI9/rclQDrwCX1SkzE3jT63LsBPC6HAUtHKM84hNCiHaoF7Ar7PtuYFydMkOARKvTvQLIAOZ6XY5FoXM6sMzqdPuBJ70ux4JYBCktKCGEaHsSFEVZG/aa3ZRrAGcADuAHwB+tTveQ0LnxXpdjNMFHhDdbne5zoxP28SRBCSFE2+PTdX1s2KtuC2cP0Cfse+/QsXC7gSVel6PM63IUAquAUQBel2NP6L0AWEzwkWHUySM+IYRof3KBwVanuz/BxDSdYJ9TuLeBf1ud7gQgieAjwH9Zne40wOR1OUpCn6cAD8QiyHbfgqrZt4+jH3xgdBhCCHHMkfIa3v12L2VVvphc3+ty+IBbgCWABrzmdTk2Wp3uOVane06ojAZ8AHwHrCE4FH0D0A341Op0fxs67va6HDH5Q1TRdT0W1zVEWlqa3tiJuoXzn+TgI48w+IvPScjMjFFkQgjRMF3X2bTvKCs2H2TF5gLW7ThEQIcFPz+DKcO7N/p6iqKU67qeFoNQW1S7f8RnyQk+Oi3PzaXDlCkGRyOEaC9KKmv4LK+Q5Z6DrNhSwIGjVQCM6NWBmycPYtLQrozu08ngKI3V7hNU6ojhKCkplK+RBCWEiB1d19laUMpyTwErNh8k11uML6CTkZLAuYOzmTQ0m4lDs+makWJ0qHGj3ScoJSmJ1NNHU56ba3QoQog2przax+d5RSzfHExKew4Hl8izdc9g1oQBTB6azZh+mSSa2/1wgHq1+wQFYLHbKXzs3/gPH8bcqX03qYUQTafrOtsLy1ge6kv6Mr+Yan8AS5KZ8YOyuOW8QUwamk2PjrLrQiQkQQFpOTkU6jrl69aRcf75RocjhGhFKmv8fJFfxMrNB1m+uYAdReUADOqaztVn9WOyrStjrZkkJ5gNjrT1kQQFpIwciZKcTPmaNZKghBCntLOonBVbCljuKeDzbUVU+QKkJJo4e2AWs8b3Z9LQrvTpbDE6zFZPEhRgSkoiddQoyqQfSghRjyqfn9zth1i+uYDlmwvIPxicztKvi4UZOX2ZNDSbMwd0ISVRWknRJAkqxGK3UzhvHv6jRzF36GB0OEIIg+09XMGK0GO7z/IKKa/2k2Q2MW5AZ342Lvjorn9Wq59qFNdilqA0m9oHWERw1rEOLFA92tw6ZRSCG2ZdDJQD16ge7avQueM201I92gmbaUWTJScHHn882A81eXIsf0oI0QBd11n89R4+2VpIQNcJ6BDQdXRdJxDg2DFd1+ucrz0Xfv748roeYf0A+AKBY/OSenVK5cdjejFpSFfOHtQFS5L8vb6lxPKftA/4jerRvtJsagawTrOpH6oebVNYmYuAwaHXOOAJYJxmU2s307qQ4IKFuZpNfadO3ahKHTUSJTGR8jW5kqCEMMCOojLuWbyez/KK6JqRjCXJjElRUBQwKcrxn03BdwVQFAVTnTJmk+n7Msedr7+8KexY7flBXdM5z9aVQV3TURTF6H887VLMEpTq0fYB+0KfSzSbqhHcgyQ8yVwGLFI9mg6s1mxqJ82m9gCsQJ7q0fIBNJtau5lWzBKUKSWFlFEjZT6UEC3M5w+w8NPt/OvDLSSaTfzlhyOYmdMXk0mSQnvXIm1VzaZagdOBL+ucqm/TrF4NHK+7mRYAoX1OZgMkJSU1K06L3U7Rkwvwl5ZiTk9v1rWEEKe2fvcR7nrjOzbtO8qUYd144LIRdO8oKymIoJhPX9ZsajrwBnC76tGORvv6uq4vqN3zJCGhefk2LScHAgEqvvoqStEJIepTXu3jL+9t4rLHP+VgaRXzfzaGBVePleQkjhPTFpRmUxMJJqcXVY/2Zj1FGto0K7GB4zGVOno0JCZSvmYN6efGZINIIdq9VVsOcs/i9ew+VMGMnL44L7LRMTXR6LBEHIrlKD4F+A+gqR7tnw0Uewe4JdTHNA44onq0fZpNPQgM1mzqyTbTijpTaiqpI0bIfCghYqC4rJo/v7eJxV/vYUBWGq/OPpNxA7oYHZaIY7FsQZ0D/BxYr9nUb0LH7gH6AqgebT7wPsEh5nkEh5n/MnTOp9nU2s20zMDTqkfbGMNYj7HY7RT95z8EysowpckcByGaS9d13vpmDw+8u4mSSh+3njeImycPkkmt4pTa/YaFdZV++hm7Zs2iz8KFpI8/J0qRCdE+7Sou5/dvbWDVloOc3rcTrh+PZGj3DKPDavNkw8I2ynL6aDCbg/1QkqCEaBKfP8Czn3v5x9ItmBS4/9Lh/OzMfphl6LhoBElQdZjS0kgZMVzmQwnRRBv3HsH5xnrW7znC+bau/PmHI+jZSbaXEI0nCaoeaXY7Rc8tIlBRgSlV/scSsefzB3h5zU4e/TiPLmlJTBnWjSnDuzO8Z4dWs4pBRbWfRz7awsJPtpNpSeTfM0/HcVqPVhO/iD/SB1WP0lWr2DX7Bvo+8zRpZ50VhciEaNgX24q4/92NePaXkNO/MwqQ6y0moAfXgbtwWDemDO9GjrUzCXG68+pneYXcs3g9O4rKuWJsb+65WKWTpXkT50XTSR9UG5Y6ZgyYTJStWSMJSsTMnsMV/J9bw71+H706pfLEVWOYOqI7iqJQXFbNR9oBlm46wMtrdvLs5146WRI5z9aVKcO6M3FINqlJxo+CO1xezYNujf+u2421i4WXrh/H2QOzjA5LtBHSgmrA9st/ipKSjPWFF6JyPSFqVdb4eXJlPk+szAPgxomDuGHigAaHXZdX+1i1pZClm/bzkVbAkYoaUhJNTBiczZRh3Thf7UbntJZtrei6zrvf7eOBdzdyqLyGG84dwG3nD5ah43GirbSgJEE14MBDf+PQCy8wJHcNphRZfkU0n67rfLBhP39xa+w5XIFjZA/uuVilVyMGEPj8AdZ4i1m68QAfbjrAnsMVmBSwWzszZXh3pgzrFvOdXPccruAPi9ezfPNBRvXuyF9/PJJhPWUPtXgSSYKyOt3HbWnkdTlO2NLI6nRPAh4huLpPodflmBhp3WiQBNWAkuXL2X3jTfR97jnSxuVE5Zqi/dq8v4T7393I59uKsHXP4N5pwzlrYPNWUdB1nY17j7J0436WbjqAZ38JAMN6dGDK8G5MGdYdtUdG1AYp+AM6i77w8vCSzeg6/PYHQ7nmbKsMHY9Dp0pQVqfbDGwhbEsjYIbX5dgUVqYT8Dkw1ety7LQ63V29LkdBJHWjRfqgGmA54wxQFMrXrJEEJZrsSHkN/1q2hedX7yA9OYE/XzacGTl9ozLYQVEURvTqyIheHbljylB2FJXx4aYDLNm4n7kfbeWRZVvpnZnKlGHdmTK8G2P7ZTb5dz37j+J8Yz3f7DrMxCHZ/OWHI2LeUhMxlQPkeV2OfACr013flkYzgTe9LsdOAK/LUdCIulEhCaoB5g4dSFZtMh9KNIk/oPNK7k7+vmQzRypquGpcP+64cAiZMewr6tcljVkTBjBrwgAKS6uCgyw2HuCFL3fw9Gfb6ZyWxPm2rkwZ3p0Jg7Mi6i+qrPHz74/zmL9yGx1SE5k7fTSXjuopQ8fjX4KiKGvDvi/QdX1B2PdItjQaAiRane4VQAYw1+tyLIqwblRIgjqJNLudQ6+8SqC6GlMz95oS7cea7cXc985GNu07yrj+nbl32vAW76PJSk/mSntfrrT3pazKx8otB1m6cT8fbNzPf9ftJjXRzLlDspgyrDvnq13rHRK+Or+Ie95cT35hGT8e04s/OIa1+GAM0WQ+XdfHNvMaCcAZwPlAKvCF1ele3ezIGhmAaIAlJ4fi5xZR+d13WMY299+1aOv2Hq7gr//z8O63e+nZMSVuJqqmJSdw8Wk9uPi0HtT4A3yZX8zSTftZuvEASzYewGxSGNe/M1OGdePC4d1JT0rgr//TeCV3F306p/L8dTlMGJxt6D2IqGtoq6Nwu4Eir8tRBpRZne5VwKjQ8RbZDkkGSZyE//Bhtpx1Nlm33kL2TTdF7bqibams8bPwk3weX76NgK5zw8SB3DhxYFzMUzqZQEBn/Z4jx5LV1oJSACxJZipr/Fw/YQC3XzAk7u9DnCiCQRIJBAc6nE8wueQCM70ux8awMirwb+AHQBKwhuDWR55T1Y3afUiCOrn8y36IuXMm/Z55JqrXFa2fruss3XSAv7g3sau4gotGdOeei9VWO3gg/2ApH246wOYDJVx7Tn9G9OpodEiiiSIcZn4xwSHkZuBpr8vxoNXpngPgdTnmh8r8juA2SAGCw8kfaahuTO5DEtTJ7f/Lgxx+/XWGrvkSRfqhRMjWAyXc/+4mPs0rZEi3dO6bNpyzB8kKCiI+tJWJutIHdQqWHDuHXniBig0bsYw53ehwhMGOVNQwd9lWnvvCS1qSmfumDeNnZ/aL2zXyhGjNJEGdgsVuB6B8zRpJUO2YP6Dz2tpd/H3JZorLq5mR05ffXDiELunJRocmRJsVswSl2dSngUuAAtWjjajn/O+Aq8LiUIFs1aMVazbVC5QAfsCnejTDhtAlZGaSPHhQcD7UnBuMCkMYaK23mPve3ciGPUexWzN5blqO9M8I0QJi2YJ6luAIkEX1nVQ92sPAwwCaTZ0G/Fr1aMVhRSarHq0whvFFzGK3c/itt9FralASE40OR7SQ/Ucqcf1P461v9tK9Q4pMUhWihcXswbnq0VYBxacsGDQDeDlWsTSXJScHvbycyk1RX8lDxKEqn5/Hl+dx3j9W8P6G/dwyeRAf/3Yil43uJclJiBZkeB+UZlMtwFTglrDDOrBMs6l+4EnVoy2otzKgKMpsYDZAUoxG2dVO0i1bs4bUUaNi8hvCeLqu85FWwJ/dm9hRVM6UYd34g2MYfbu0zmHjQrR28TD0aBrwWZ3He+NVjzYauAi4WbOp5zZUWdf1Bbquj9V1fWxCQmzybUJWFkkDBsi6fG3c79/awKxFa0k0m3j+uhwWXD1WkpMQBoqHBDWdOo/3VI+2J/ReACwmuHquoSw5dirWfYXu8xkdioiBt77ew0tf7uTac/rzv19NkKV9hIgDhiYozaZ2BCYCb4cdS9NsakbtZ2AKsMGYCL9nsdsJlJVRqXmMDkVE2c6icv7w1gbs1kzuudhGosxpEiIuxHKY+cvAJCBLs6m7gXsJ7sqI6tHmh4r9CFiqerTw5R+6AYs1m1ob30uqR/sgVnFGKnw+VOppJ4yaF61UjT/Ara98jUmBR6afLhNuhYgjstRRI2z7wVSS+venz/wnYvYbomX97QMP81ZsY95VY7j4tB5GhyNEVLSVpY7kr4uNYMnJoXzdOnS/3+hQRBR8nlfIEyu3Md3eR5KTEHFIElQjWHLsBEpKqNq82ehQRDMVl1Xz69e+oX9WGn+aNszocIQQ9ZAE1Qi1/VBla9YYHIloDl3XufP17zhUVsOj00/HkmT4dEAhRD0kQTVCYvfuJPbpQ3nuWqNDEc3wwuodLNMOcOfUobKmnhBxTBJUI1ly7FSsXYseCBgdimiCzftL+ItbY+KQbK49p7/R4QghTkISVCNZ7Hb8R45QtXWr0aGIRqqs8XPby1+TkZLI3386CpNJ1tUTIp5JgmqktNr5UF9KP1Rr83/va2w+UMI/rhhFdobs4yREvJME1UiJvXqR2LOnrMvXyny46QCLvtjBrPH9mThEljESojWQBNUElpwcyteupS1Ncm7L9h+p5M7Xv2V4zw78bupQo8MRQkRIElQTWOx2/IcOUZ2XZ3Qo4hT8AZ07XvuGypoAj844neQEs9EhCSEiJAmqCSw5Mh+qtXhy1TY+31bE/ZcOZ2B2utHhCCEaQWYoNkFi794kdO9Oee5aOl91ldHhiAZ8s+sw/1y6BcfIHvx0bG+jwxEirlid7qnAXMAMLPS6HK465ycR3Glie+jQm16X44HQOS9QAvgBn9flGBuLGCVBNYGiKFhy7JR99jm6rss24HGopLKG217+mm4dUvi/H50m/46ECGN1us3A48CFwG4g1+p0v+N1OTbVKfqJ1+W4pIHLTPa6HIWxjFMe8TWRxW7HX1RE9fbtpy4sWtyf3t7I7kPlzJ0+mo6piUaHI0S8yQHyvC5HvtflqAZeAS4zOKYTSAuqidLC9odKHjDA4GhEuMVf72bx13u4/YLBjLV2NjocIYyQoChK+JpsC3RdXxD2vRewK+z7bmBcPdc52+p0fwfsAX7rdTk2ho7rwDKr0+0HnvS6HAvqqdts0oJqosR+/UjIzqZ8jcyHiic7isr4w+Lg7ri3TB5kdDhCGMWn6/rYsFdTEshXQF+vyzESeAx4K+zceK/LMRq4CLjZ6nSfG4WYTyAJqomC/VA5lOfmynyoOFHjD3DbK99gNimyO64QJ7cH6BP2vXfo2DFel+Oo1+UoDX1+H0i0Ot1Zoe97Qu8FwGKCjwyjLpZbvj8NXAIUqB7thD3SNZs6iTojRFSP9kDo3HGjS1SP5qpbPx5Y7HaOut3U7NhBktVqdDjt3r8+3MK3uw4z76ox9OqUanQ4QsSzXGCw1enuTzAxTQdmhhewOt3dgQNel0O3Ot05BBs0RVanOw0weV2OktDnKcADsQgyln/FfBaYeooyn6gebXToVZucakeXXAQMA2ZoNjUud5ST+VDxQ3bHFSJyXpfDB9wCLAE04DWvy7HR6nTPsTrdc0LFLgc2WJ3ub4FHgelel0MHugGfho6vAdxel+ODWMSpxPLxlGZTrcB7J2lB/Vb1aJfUOX4WcJ/q0X4Q+n43gOrR/nqq30tLS9PLysqiEHlkdF1n64RzSTvrLHo9/LcW+11xvOKyai6au4r05ATevXW8bEAo2j1FUcp1XU8zOo7mMvr/5LM1m3pshIjq0TYS+egSABRFmQ3MBkhKSophqPX+Nhb72GP9UDLXpuWF74779DV2SU5CtCFG9iJ/BfRVPVp9I0Qipuv6gtqRKgkJLf+Hk8Vux7d/PzW7d7f4b4vvd8e96yIbw3vK7rhCtCWGJSjVox1VPVpp6PP7QKJmU7OIYHRJPAmfDyVaVu3uuJOGZnPtOVajwxFCRJlhCUqzqd01m6qEPh8bIUJodIlmU/trNjWJ4OiSd4yK81SSBg3CnJkp86FaWGWNn1tf/urY7rjyeFWItieWw8xfBiYBWZpN3Q3cCyQCqB5tPsERIjdqNtUHVADTVY+mAz7NptaOLjEDT4f6puJSsB/KLhsYtrAH3RpbDpSy6NocstJld1wh2qKYjuJraS09iq9W8fMvcODBBxm4bBlJvXu1+O+3N0s37mf28+u4fkJ/fu+IyxkIQhiqrYzik6n2UVA7H0paUbG3/0gld77xHcN7duC3P5DdcYVoyyRBRUHy4MGYO3aUBBVjtbvjVsnuuEK0CzJpJAoUk4nU0HwoETu1u+P+7ScjZXdcIdoBaUFFSZrdTs2uXdTs22d0KG2S7I4rRPsjCSpKLHbph4oV2R1XiPZJElSUJA8diikjQxJUDMjuuEK0T5KgokQxm7GMHSsTdqOsdnfcX50/RHbHFaKdkQQVRRa7neodO6g5UGB0KG1C7e64OdbO3HKe7I4rRHsjCSqKpB8qesJ3x/3X9NGYTdLvJER7IwkqilJUG6a0NElQUfDP0O64D/1kpOyOK0Q7JQkqipSEBFLHniEJqpk+zytk/sptzMjpw0WyO64Q7ZYkqChLs9upzs/HV1hodCitUnFZNb9+7RsGZKXxx0tknT0h2jNJUFEm/VBNF7477qMzTpfdcYVo5yRBRVnKsGEoFoskqCaQ3XGFEOEkQUWZkpiIZcwYSVCN9O2uw7I7rhDiOJKgYsBit1O1NQ9fcbHRobQKyzcXMOOp1WSlJ8vuuEKIY+Qhfwx83w+1lg4/mGJwNPHt1dyd3LN4A7buGTxzjV12xxWihVid7qnAXII7ly/0uhyuOucnAW8D20OH3vS6HA9EUjdaYrnl+9PAJUCB6tFG1HP+KuAuQAFKgBtVj/Zt6Jw3dMwP+FSPNjZWccZC6ojhKCkplOfmSoJqgK7rPLJsK3M/2sqEwVk88bMzSE+Wvy8J0RKsTrcZeBy4ENgN5Fqd7ne8LsemOkU/8boclzSxbrPF8hHfs8DUk5zfDkxUPdppwJ+BBXXOT1Y92ujWlpwAlKQkLGNOl36oBtT4A9z1xnfM/Wgrl5/Rm6evsUtyEqJl5QB5Xpcj3+tyVAOvAJfFoq7V6f4/q9PdKex7ptXp/kskPxSzBKV6tFVAg50wqkf7XPVoh0JfVwNtapMfi91O1ZYt+A8fNjqUuFJW5WPWc2t5be1ubjtvEA9fPpJEs3SFChFlCYqirA17za5zvhewK+z77tCxus62Ot3fWZ3u/1md7uGNrFvrIq/LcewPQq/LcQi4OJKbiJc/Ga4D/hf2XQeWaTZ1nWZT6/6DPY6iKLNr/yX4fL6YBtkYFrsddJ3ytWuNDiVuFJRUcuWCL/g0r5C//vg07pgyVAZECBEbPl3Xx4a96j6hisRXQF+vyzESeAx4q4mxmK1O97HOZavTnQpE1Nls+HMVzaZOJpigxocdHq96tD2aTe0KfKjZVE+oRXaC0D/4BQBpaWl6zAOOUMrIkSjJyZTn5pJxwQVGh2O4bQdL+cXTaygqreapq8/gPFs3o0MSoj3bA/QJ+947dOwYr8txNOzz+1ane57V6c6KpG4dLwIfWZ3uZ0Lffwk8F0mQhiYozaaOBBYCF6keraj2uOrR9oTeCzSbupjgM896E1S8MiUlkTp6NGXSD8W6HcVc99xazIrCK7PPZFSfTqeuJISIpVxgsNXp7k8wuUwHZoYXsDrd3YEDXpdDtzrdOQSfuBUBh09VN5zX5XjI6nR/C9T+Tf3PXpdjSSRBGvaIT7OpfYE3gZ+rHm1L2PE0zaZm1H4GpgAbjImyeSx2O1WaB//Ro6cu3EZ9sGEfM5/6kkxLEm/edLYkJyHigNfl8AG3AEsADXjN63JstDrdc6xO95xQscuBDaHk8igw3ety6A3Vbei3Qolshdfl+K3X5fgtsMrqdFsjiVPR9dg8FdNs6svAJCALOADcCyQCqB5tvmZTFwI/AXaEqvhUjzZWs6kDgMWhYwnAS6pHezCS30xLS9PLysqidxPNVPblGnb+4hf0njePjPMmGx1Oi3v2s+3c/94mRvfpxMKrx9JF5jgJ0SIURSnXdT3N6DgArE73WuDs0Ig/rE53EvCZ1+Wwn6puzB7xqR5txinOzwJm1XM8HxgVq7haUuqokSiJicF+qHaUoAIBnYc+8PDkqnwuHNaNR6efTmqS2eiwhBDGSKhNTgBel6M6lKROKV5G8bVJppQUUkeNalfzoap8fn716jc8uSqfn53Zl/k/O0OSkxDt20Gr031p7Rer030ZENF+RIaP4mvrLDl2Cuc/ib+0FHN6utHhxNSRihpueH4tq/OLuWuqjTkTB8gwciHEHOBFq9P9b4IrB+0Cro6kYsz6oIwQb31QAGVffMHOX15Lnyfnkz5xotHhxMzewxVc88watheW8fDlo/jh6SebtyeEiKV46oOqZXW60wG8LkdppHUiakFpNvVXwDME18dbCJwOOFWPtrQJcbYrqaNHQ6gfqq0mKG3fUX75TC5lVT6e/WUO5wzKMjokIUQcsTrdDmA4kGJ1ugGoXXj2ZCLtg7pW9WhHCQ75zgR+DsRk9dq2xpSaSuppp7XZ+VCf5RVyxfwvAHhtzlmSnIQQx7E63fOBK4FbCT7i+ynQL5K6kSao2o6Ei4HnVY+2MeyYOAWL3U7lho0E4uzxY3O99fUernlmDT06pfDmTWej9uhgdEhCiPhzttfluBo45HU57gfOAoZEUjHSBLVOs6lLCSaoJaGJtIEmhdoOWex28Psp/+pro0OJCl3Xmbcij9tf/YYz+mXy3zln07NTqtFhCSHiU0XovdzqdPcEaoAekVSMNEFdBzgBu+rRyglOuP1lY6NsryynjwazuU0MN/cHdP749gb+9sFmpo3qyXPX5tAxNdHosIQQ8eu90HYbDxNcgNYLvBRJxUiHmZ8FfKN6tDLNpv4MGENwN0URAVNaGqkjRrT6BFVR7ee2V77mw00HuOHcAdw11YbJJE96hRAN87ocfw59fMPqdL8HpHhdjiOR1I00QT0BjNJs6ijgNwRH8i0C2uawtBiw5NgpevY5AuXlmCwWo8NptOKyaq57Lpdvdh3mvmnDuOac/kaHJIRoZbwuRxVQFWn5SB/x+VSPphPcNfHfqkd7HMhoQnztlsVuh5oaKr75xuhQGm1HURk/eeJzNu09yhNXjZHkJIRoEZEmqBLNpt5NcHi5W7OpJkILv4rIpI4ZAyZTqxtu/u2uw/x43uccKq/mxVnjmDoior5NIYRotkgT1JUEm2XXqh5tP8ENqh6OWVRtkDk9nZThw1tVP9RH2gGmL1hNapKZN248m7HWzkaHJIRoZaxO90eRHKtPRH1Qqkfbr9nUFwG7ZlMvAdaoHm1R48IUFrudQ88/T6CyElNKitHhnNRLX+7kD2+tZ1jPDjx9jZ2uGfEdrxAivlid7hTAAmRZne5Mvp872wGIaC20iFpQmk29AlhDcAbwFcCXmk1RYB7NAAAgAElEQVS9vNERt3MW+1j0mhoqvvnW6FAapOs6/1i6mXsWr2fC4GxenX2WJCchRFPcAKwDbKH32tfbwL8juUCko/h+T3AOVAGAZlOzgWXA640MuF2znHEGKArlubmknTnO6HBOUOMP4HxjPW98tZsrxvbmwR+dRqJZdmQRQjSe1+WYC8y1Ot23el2Ox5pyjUj/9DHVJqeQokbUFSHmDh1IUdW47IcqrfJx7bO5vPHVbm6/YDAP/WSkJCchRDTstzrdGQBWp/sPVqf7TavTPSaSipG2oD7QbOoS4OXQ9yuB909WQbOpTwOXAAWqRxtRz3mF4GTfi4Fy4BrVo30VOjc1dM4MLFQ9WptZmNZit3PolVcIVFVhSo6PLdCLSqu4+uk1ePaX8LefjOQKex+jQxJCtB1/9Loc/7U63eOBCwgOsHsCOOVjpIj+iqx6tN8BC4CRodcC1aPddYpqzwJTT3L+ImBw6DWbYMBoNtUMPB46PwyYodnUYZHE2RpYcuzoVVVUfved0aEAcLCkihlPrSavoJSFV4+V5CSEiDZ/6N0BLPC6HG4goi3fI95RV/VobwBvNKL8Ks2mWk9S5DJgUWgC8GrNpnbSbGoPwArkqR4tH0Czqa+Eym6K9LfjWW0/VFlubnDyroEKSiqZ+dSX7D5UzjPX2DlbtsoQQkTfHqvT/SRwIfCQ1elOJsLG0UkTlGZTS4D6ttxVAF31aM3ZX6EXwa1/a+0OHavveINNQUVRZhNsgZGUFFFSNpS5UyeShw41vB+q4GglM55azd7DlTxzTQ5nDexiaDxCiDbrCoJP0/7udTkOW53uHsDvIql40gSlerS4X85I1/UFBB8/kpaW1ir2r7fY7Rz+73/Rq6tRDEiq+49UMvOp1ew/Wslz1+aQ018m4ArR3lid7uP6+r0uR719/Van2w58AUz3uhyvh455Ce6w7gd8XpdjbEO/43U5yq1OdwEwHtgK+ELvp2TkMK09QHiHR+/QsYaOtxkW+1j0ykoqNmxo8d/ed6SC6Qu+4MDRShZJchKiXbI63Sf09Vud7hP6+kPlHgKW1nOZyV6XY/TJklPoGvcCdwF3hw4lAi9EEqeRCeod4GrNpiqaTT0TOKJ6tH1ALjBYs6n9NZuaBEwPlW0zavueyte07GO+PYcruPLJ1RSWVrPounGydJEQ7VcOkOd1OfK9Lkc1UNvXX9etBMceFNRzLlI/Ai4FygC8LsdeIlxsPGYJSrOpLxNsFg7VbOpuzaZep9nUOZpNnRMq8j6QD+QBTwE3AagezQfcAiwBNOC10BbzbUZCZibJgwe3aD/U7kPlTF/wBYfKqnn+uhzO6JfZYr8thGhxCYqirA17za5zvqExAMdYne5eBJPLE/VcXweWWZ3udVanu+6166r2uhx6qA5Wpzst4puItGBjqR5txinO68DNDZx7n1PMs2rtLHY7h996C72mBiUxtgvD7youZ/qC1ZRU1vDCrHGM6tMppr8nhDCcT9f1kz56i8AjwF1elyNgdbrrnhvvdTn2WJ3ursCHVqfb43U5VjVwnddCo/g6WZ3u64FrCe4peEqyVIBBLDl29PJyKjfGtnG4o6iMK5/8gtIqHy/OOlOSkxACIuvrHwu8EhoQcTkwz+p0/xDA63LsCb0XAIsJPjKsl9fl+DvBZfHeAIYCf/K6HI9GEmTMWlDi5Cxjg3+5KcvNJXX06Jj8hrewjBlPraaixs+Ls8YxolfHmPyOEKLVyQUGW53u/gQT03RgZngBr8txbGdSq9P9LPCe1+V4K/SIzuR1OUpCn6cADzT0Q1an+yGvy3EX8GE9x05KWlAGScjKImngwJj1Q+UfLOXKBV9QWePnpVlnSnISQhzjdTlO6Ov3uhwbrU73HKvTPefktekGfGp1ur8luMuF2+tyfHCS8hfWc+yiSOJUdL1VTB2KSFpaml5WVmZ0GBHbd999HH33PYZ8uRolIXqN2byCUmY+tRp/QOfF68dh696c+dRCiNZGUZRyXdcjHowQC1an+0aCg98GANvCTmUAn3ldjp+d6hrSgjKQxW4nUFZGpaZF7ZpbD5QwfcFqArrOy7PPlOQkhDDKS8A0gtOEpoW9zogkOYH0QRkqfD5U6mmnNft6m/eXcNXC1SiKwsvXn8mgrnG/EIgQoo3yuhxHgCPASUd0n4y0oAyU2LUrSVZrVPqhtH1HmfHUakyKwiuzJTkJIVo/SVAGs9jtlK9bh+73n7pwAzbuPcLMp1aTZDbx6g1nMTA7PYoRCiGEMSRBGcySYydQUkKlx9Ok+hv2HOGqhV+Skmjmldln0j/L0H5RIYSIGklQBjvWD9WEx3zrdwdbTmlJCbw6+yyskpyEEG2IJCiDJXbvTmLfvhx1v0+gujriet/sOszMhavpkJrIK7PPpG8XSwyjFEKIlicJKg5k33YblevXs/d3d0bUF/XVzkP8fOGXdLIEk1OfzpKchBBtjySoONDxEgddnXdRsmQJ+x/4MyebPL1uRzFX/2cNndOTeHX2WfTOlOQkhGibZB5UnOhyzTX4i4opeuopErp0Ifu2W08ok+st5pqn19C1QwovX38m3TumGBCpEEK0DElQcST7jl/jKy6icN48zJ070/lnVx0792V+Eb98NpfuHYPJqVsHSU5CiLZNElQcURSFHvffj//wEQ48+CDmzE50dDj4YlsR1z6bS6/MVF6aNY6ukpyEEO2ALBYbhwKVleyadT3l337Lnj8/yi3rKujb2cKLs84kOyPZ6PCEEHEuHhaLjYaYJijNpk4F5gJmYKHq0Vx1zv8OqH2OlQCoQLbq0Yo1m+oFSgA/4FM92il3h2wrCQrAX1LC6zfczR97XYC1i4WXbplIVrokJyHEqUmCOgXNppqBLQT3AtlNcIOsGapH29RA+WnAr1WPdl7ouxcYq3q0wkh/sy0lqBWbC5i9aC29Swp46OtFjHxuIckDBhgdlhCiFWgrCSqWw8xzgDzVo+WrHq0aeAW47CTlZwAvxzCeVmO5p4DZi9YxqGsGL99yLh0D1eycNYua/fuNDk0IIVpMLBNUL2BX2PfdoWMn0GyqBZhKcM/6WjqwTLOp6zSbOjtmUcaZZZsOcMPz6xjaPYOXrh9HtyED6PvUAgJHjrJz1iz8hw8bHaIQQrSIeJmoOw34TPVoxWHHxqsebTTBrYFv1mzqufVVVBRltqIoaxVFWevz+Voi1phZunE/N764DrVHBi9cN45OliQAUoYNo/e8edTs2MmuOTcSKC83OFIhhIi9WCaoPUCfsO+9Q8fqM506j/dUj7Yn9F4ALCb4yPAEuq4v0HV9rK7rYxOiuG16S/tgwz5uevErhvfsyKLrxtHRknjc+bRxOfT8x9+p+O47dt9+O3pNjUGRCiFEy4hlgsoFBms2tb9mU5MIJqF36hbSbGpHYCLwdtixNM2mZtR+BqYAG2IYq6E+2LCPm1/6mpG9O/L8dTl0TE2st1yHKVPoft+9lK36hL2//z16INDCkQohRMuJWZND9Wg+zabeAiwhOMz8adWjbdRs6pzQ+fmhoj8ClqoeLXz4XTdgsWZTa2N8SfVoH8QqViOVVNZw1xvrGdEr2HJKTz75v5LMK67AX3yIg488QkJmJl2dThRFaaFohRCi5chEXYM99tFW/vHhFt655RxG9u4UUR1d1znw179yaNHzZN9xB1mzr49xlEKI1qStDDNvvZ02bcDRyhoWfrqd821dI05OEFwSqZvTGWxJ/fOfmDM7kfnTn8YwUiFEW2N1uo9bSMHrcrgaKGcHvgCme12O1xtTt7niZRRfu/TsZ16OVNRw+wVDGl1XMZno+X8PkjZhAvvvvY+SZctiEKEQoi2yOt1m4HGCo6SHATOsTvewBso9BCxtbN1okARlkCMVNSz8JJ8L1G6c1rtjk66hJCXRe+4jpJw2gj13/IayNWuiHKUQoo3KAfK8Lke+1+U42UIKtxKcn1rQhLrNJgnKIM98tp2jlT5uv2Bws65jsljoM38+iX36sPumm6nUtChFKIRow065kILV6e5FcBDbE42tGy2SoAxwpKKG/3y6nSnDujGiV9NaT+ESMjPpu/ApTBkZ7Lx+NtU7d0YhSiFEK5ZQu4BB6NWU1XgeAe7yuhyGzWeRQRIGePrT7ZRU+vhVM1tP4RJ79KDvwqfYMfMqdl43C+tLL5KQnR216wshWhWfrusn2wEikoUUxgKvWJ1ugCzgYqvT7YuwblTIMPMWdqS8hvEPfczZg7rw5M9PuYNIo1V8+y07rvklSf360e/5RZgzMqL+G0KI+HaqYeZWpzuB4G4T5xNMLrnATK/LsbGB8s8C73ldjtcbW7c55BFfC/vPp/mUVPmaNHIvEqmjRtH70Uepystj9403EaiqisnvCCFaL6/L4QNqF1LQgNe8LsdGq9M9x+p0z2lK3VjEKS2oFnS4vJrxDy1nwuAsnvjZGTH9rSPvudn729+SfsH59H7kEZRWvE6hEKJx2spEXWlBtaD/fLqd0ioft50fvb6nhnS8xEG3e+6hdNlH7LvvPtrSX0SEEO2D/LW6hRwur+aZz7xcfFp31B4dWuQ3O1/9c3yHiil6Yj4JXbLo+uvbW+R3hRAiGiRBtZCnPsmnrNrHr86PTd9TQ7Jvuw1/UTFFTz5JQudMOv/iFy36+0II0VSSoFpAcVk1z37m5eLTejC0e8uOqlMUhe73/gn/oUMc+KsLc+fOdJw2rUVjEEKIppA+qBaw8JN8ymv8/KoF+p7qo5jN9Pz7w1jGjWPv3fdQumqVIXEIIURjSIKKseKyap773IvjtB4M6WbcnCRTcjK9H/83yUMGs/u2X1H+9deGxSKEEJGQBBVjC1YZ23oKZ05Pp++CBSR068quOTdStXWr0SEJIUSDJEHFUFFpFYu+8DJtZE8GG9h6CpeQlUXf//wHJSmRnbOup2bvXqNDEkKIekmCiqEFn+RTWeNvkXlPjZHUuzd9Fy4kUF7Ozutm4Tt0yOiQhBDiBDFdSUKzqcftuqh6NFed85OAt4HtoUNvqh7tgUjq1ieeVpIoLK1iwkPL+cHwbjwy/XSjw6lX+dq17LxuFslDhtDv2WcwpbX6iedCCGQliVPSbOoJuy5qNrW+XRc/UT3a6NDrgUbWjVsLVuVT5fNza5y1nsJZxo6l17/+SeWmTey+9TYC1dVGhySEEMfE8hFfDpCnerR81aM1dtfF5tQ13MGSYN/TZaN7MTA73ehwTirjvPPo8cADlH3+OfmOSzjyzjvofr/RYQkhREwTVKS7Lp6t2dTvNJv6P82mDm9kXRRFmV27KZfP54tG3M325MptVPsC3HreIKNDiUinn/yYPk89hSk9nb133sX2H/6Iko8/lvX7hBCGMnqQxFdAX9WjjQQeA95q7AV0XV+g6/pYXdfHJsTBit0FJZW88OUOfnh6LwbEeespXPqE8fR/43V6/fMf6NXV7L7pZnbMmEnZl2uMDk0I0U7FMkGdctdF1aMdVT1aaejz+0CiZlOzIqkbr55cmU+NX+fW8+K376khislEh4svZsB779L9gfup2bePnb/4BTuvm0XFhphs9yKEEA2KZYLKBQZrNrW/ZlOTgOnAO+EFNJvaXbOpSuhzTiieokjqxqOCo5W8sHoHPxzdi/5ZrXcAjZKYSOYVVzBwyQd0vfNOKjduxHv55ez+1e1U5ecbHZ4Qop2IWYJSPdoJuy6qHm2jZlPnaDa1dsfGy4ENmk39FngUmK56NL2hurGKNVqeWLkNX0DntvNbR9/TqZhSUuhy7S8ZuOxDsm66ibJPPiH/kmns/f3vZYKvECLmZEfdKCk4WsmEvy3n0lE9efinowyJIdZ8xcFtOw699DIAmTNn0OWGG0jo3NngyIQQ4WQelDjOvBXB1lNr7HuKVELnznS7+24GLvmADpdOo/j5F9h2wYUcfPQx/KWlRocnhGhjJEFFwf4jlby0Zic/GdOLvl0sRocTc4k9e9LzwQcZ8N67pE2YQOG8eWy74EKKnn6GQGWl0eEJIdoISVBR8MSKPAJtvPVUn+QBA+g99xGsr79OyvDhFPztb2z7wVQOvfYaepzMSRNCtF6SoJpp/5FKXl6zi8vP6E2fzm2/9VSf1BHD6fufhfR97jkSu3dn/5/uJd9xCUfffx89EDA6PCFEKyUJqpnmrcgjoOvcPLltjNxrjrRxOfR75WV6z3scJSmJPXf8hu2XX07pqlWyKoUQotFkFF8z7D1cwaSHV/CTM3rx1x+PbLHfbQ10v5+jbjcHH32Mmt27SR17Bl3vuAPLmDFGhyZEmxfJKD6r033cjhFel8NV5/xlwJ+BAOADbve6HJ+GznmBEsAP+Lwux9io3wTSgmqWeSvy0JHWU30Us5mOl17KwPfddPvTH6nesYMdM69i1w1zqPR4jA5PiHbN6nSfsGOE1emuu2PER8Aor8sxGrgWWFjn/GSvyzE6VskJJEE12Z7DFbyau4ufju1D78z22fcUCSUpic4zZzJoyRKyf3MH5V9/zfYf/og9v/kt1Tt2GB2eEO1VDpDndTnyvS5HvTtGeF2OUq/LUfuILQ1o8cdtkqCaaN7yPABpPUXIZLGQdf31DFr2IV1uuIGSjz9mm+MS9t17HzUHDhgdnhBtTULtLg+h1+w65yPaMcLqdP/I6nR7ADfBVlQtHVhmdbrXWZ3uuteOGklQTbD7UDmvrd3FlfY+9OqUanQ4rYq5Qwe6/vp2Bi1dQuaVV3L4zTfZNuUHHHj4Ydl6Xojo8dXu8hB6LWjKRbwux2Kvy2EDfkiwP6rW+NCjv4uAm61O97lRiPkEkqCa4PHl21BQuGmStJ6aKiE7m+5//AMD//c+HaZOpfjpZ8g773x23XwLh157TVpVQsRWo3aM8Locq4ABVqc7K/R9T+i9AFhM8JFh1Bm/gVIrs6u4nP+u3cWMnL70lNZTsyX17k3Ph1x0mXUdxS+9ROnKlZR+9BEAyapK+sRzST93IqmjRqKYzQZHK0SbkQsMtjrd/QkmpunAzPACVqd7ELDN63LoVqd7DJAMFFmd7jTA5HU5SkKfpwAPxCJIGWbeSHe/+R1vrNvDyjsn0aOjJKho03Wdqq1bKV25krKVqyj/+mvw+zF36kTahAmkT5xI+vhzMHfqZHSoQsStCIeZXww8QnCY+dNel+NBq9M9B8Drcsy3Ot13AVcDNUAF8Duvy/Gp1ekeQLDVBMFGzktel+PBmNyHJKjI7SouZ/LfV3DVuL7cf9mImP2O+J7/yBHKPvss2LJa9Qn+Q4fAZCJ19Ohgspo0keQhQ1AUxehQhYgbbWU1c0lQjXDX69+x+Js9rPrdZLp3TInZ74j66X4/lRs2BJPVipVUbtoEQEKPHqSfey7pEyeSduY4TBYZ9i/aN0lQcSiWCWpnUTmT/7GCn5/Zj/suHR6T3xCNU3OggLJPVlG6chVln31GoLwcJSkJS07OsdZVUp8+p76QEG2MJKg4FMsEdefr3/L2N3tZdedkunWQ1lO80aurKV+3jtIVKylduZJqrxeApAEDgslq4kQsY05HSUoyNlAhWoAkqAhoNvW4tZ5Uj+aqc/4q4C5AIbiu042qR/s2dM5L2FpPqkc75XIasUpQO4rKOO8fK7n6rH7cO01aT61B9Y4dlK5cRenKlZSvWYNeU4MpLY20c84JJqxzJ5CQnW10mELERFtJUDEbZq7Z1Nq1ni4kOEs5V7Op76gebVNYse3ARNWjHdJs6kXAAmBc2PnJqkcrjFWMkXrs4zwSTAo3ThxodCgiQkn9+tH56p/T+eqfEygro2z16mDratUqSpYuBSBlxIhg39WkiaSMGIFikmmBQsSTWM6DygHyVI+WD6DZ1Nq1no4lKNWjfR5WfjXByWJxxVtYxuKv9/CLs6x0lUd7rZIpLY2M888n4/zzg8PYPZ5jravC+fMpnDcPc5cupE+YgOXMcaSoKsn9+8vjQCEMFssEVd9aT+MaKAtwHfC/sO86sEyzqX7gSdWj1btUR2iNqdkASTH4A+XRj7eSaFaYM2lA1K8tWp6iKKSoKimqStacG/AdOkTZp59SumIlJcuXc+Stt4IFExNJHjiQlKFDSbbZSLEF3xMyM429ASHakbhYSUKzqZMJJqjxYYfHqx5tj2ZTuwIfajbVo3q0VXXrhtaYWgDBPqhoxrW9sIy3vt7Dtef0p2uGtJ7aooTMTDpOm0bHadPQ/X6qt2+n0rOZqs0eKj2bKfv8c468/fb35bt2Jdk2lJShtuC7zUZSv34oCXHxv5IQbUos/6+KaK0nzaaOJLjPyEWqRyuqPa56tD2h9wLNptau9XRCgoqlxz7aSlKCiRuk76ldUMxmkgcNInnQILjEcey4r6iIqs2bj0tcRZ9/AT5fsF5yMsmDBx9LXCm2oSQPHYq5QwejbkWINiGWCSoXGKzZ1AbXetJsal/gTeDnqkfbEnY8DTCpHq0k9Dlmaz01ZNvBUt76Zg/Xje9PdkZyS/60iDMJXbqQcPbZpJ199rFjenU1Vfn5VHo8VHk2U7nZQ+lHH3Pk9TeOlUns2fP7x4OhxJXYp48MxhAiQjFLUKpH82k29RZgCaG1nlSPtlGzqXNC5+cDfwK6APM0mwrfDyfvBiwOHUsAXlI92gexirU+j320leQEs7SeRL2UpCRSbDZSbLZjx3Rdx1dw8Fgrq8rjoXLzZkpXrIBAAAjui5U8ZMixx4PJQ4eSMmQIprRWPyJYiKiTibr1yCsoZcq/VnL9hAHcfbEahchEexaorKRqa94JiStQUhIsoCgk9u1zrF8rqXdvErKzSejalYSuXTGlp8tag6JRZB5UG/bYx1tJSTQz+1wZuSeaz5SSQuppI0g97fsFhnVdx7d3L5WbNx/3mLB2jlY4JSUlmKyys0nomk1CdjaJoeR1LJFlZ2PKyJBEJtoUSVB15BWU8M63e7nh3IF0SZe+JxEbiqKQ2KsXib16kXHeeceOB8rLqdl/AN/Bg/gKCr5/D32u2qRRenAVenn5iddMSQlreYUlsrAkltC1qyQy0WpIgqpj7kd5pErrSRjEZLGQPKA/yQP6n7Scv7QM38ECfAV1Elno/aSJLDn52OPDE1pltcksKwtTx46SyIShJEGF2XqghPe+28uciQPpnCarCIj4ZU5Pw5zen+T+jUhkdVpjvoICqjweylatIlBPIiMxMTiCMSsr+MrOwtylCwlZ2ce+J3TpgjkrG1OaRZKZiDpJUGHmfrQVS6KZ2ROk9STahiYnssKD+IuK8B0sxFdYSM2BA1Rs3IC/qPjYiMRwSmpqMGl16RJMZLVJrUsokYW+m7OyMCXLo3MRGUlQIVsOlOBev4+bJg0kU1pPop2JNJHpfj/+Q4fwHUteB/EXFuIrLMJXGExm1V4vvty1+A8frvcapoyMsIQV1iLLyiIhq0swuXXujCkjA1NamrTM2jFJUCFzl20lLSmBWeOl9SREQxSz+VgyYejQk5bVq6vxFReHkldtIgtPZgep0jyUFX5KoLS0/ouYTJjS0zFnZGDKyMCcno6pQwfMGemYMjpgykjHnJ6BqUPG92UyMjClZ2DuEPwuLbbWSxIU4Nl/FPf6fdwyeZC0noSIEiUpicTu3Uns3v2UZQMVFfiKir5PYsXFBEpK8ZeWEDhaQqC0BH9JKYGjR6nZu5eqkhL8JSXBxFbPI8fj4khMDCa18ORWm9TC3zNCSa02uaWlYUpPD7bikpKkJWcASVDAox9tJSM5gVkTTv54QwgRG6bUVJJ694bejdtxRw8ECJSXE6hNWMfeS/GXHCVQUkqg5GgwuYWVqSkoIHC0BH9pab0jHU+QkBBMWGkWzGlpmCxpoe9pxyWy2jK1n81p9ZSzWFDM5ib+k2pf2n2C0vYd5f31+7ntvEF0skjrSYjWRDGZMKenY05PJ7FHjyZdQ6+pwV9aSqC0FP/Ro98nt7IyAqVlwfcGXjUFBwiUlR/7jt8fWdypqcclM3PdhBdKZh0vu5SkPn1OfcE2qt0nqOWbC8hISeA66XsSol1SEhOD+3w1c68vXdfRq6qOT2KlpfiPS2rlJ5w/luwOFhDwfl9Gr6jAYrfHLEFZne6pwFyCa6Uu9LocrjrnLwP+DAQAH3C71+X4NJK60SJr8QEHS6pkxXIhRFzRfT5QlCY9DjzVWnxWp9sMbAEuJLiZbC4ww+tybAorkw6UeV0O3ep0jwRe87octkjqRku7b0EBkpyEEHEnxptg5gB5XpcjH8DqdL8CXAYcSzJelyN8aGUawV3OI6obLZKghBCi7UlQFGVt2PcFod3Ha/UCdoV93w2Mq3sRq9P9I+CvQFegdhfPiOpGgyQoIYRoe3y6ro9t7kW8LsdiYLHV6T6XYH/UBc2OrBFka08hhGh/9gDhoy96h47Vy+tyrAIGWJ3urMbWbQ5pQQkhRPuTCwy2Ot39CSaX6cDM8AJWp3sQsC00SGIMkAwUAYdPVTdaYpqgNJt63FBE1aO56pxXQucvBsqBa1SP9lUkdYUQQjSN1+XwWZ3uW4AlBP+Mfdrrcmy0Ot1zQufnAz8BrrY63TVABXCl1+XQgXrrxiLOmA0z12xqvUMRVY+2KazMxcCtBBPUOGCu6tHGRVK3PtHa8l0IIVqztrLleyz7oHKAPNWj5aserRqoHYoY7jJgkerRdNWjrQY6aTa1R4R1hRBCtGGxTFD1DUXsFWGZSOoCoCjKbEVR1iqKstbn8zU7aCGEEPGh1Q+SCI3tXwCgKEpAUZSKJlwmgeBSHm2B3Ev8akv3I/cSn2rvJdXoQKIhlgkqkqGIDZVJjKDuCXRdb1KLUFGUtdGYMxAP5F7iV1u6H7mX+NSW7gVim6BygcGaTT3ZUMR3gFs0m/oKwUESR1SPtk+zqQcjqCuEEKINi1kflOrRfEDtUEQNeE31aBs1mzpHs6lzQsXeB/KBPOAp4KaT1Y1VrEIIIeJPm1rNvKkURZldZ52qVkvuJX61pfx3SQ0AAAdESURBVPuRe4lPbeleQBKUEEKIOCVr8QkhhIhLkqCEEELEpXafoBRFmaooymZFUfIURXEaGMfTiqIUKIqyIexYZ0VRPlQUZWvoPTPs3N2hmDcrivKDsONnKIqyPnTuUUVRlNDxZEVRXg0d/1JRFGtYnV+EfmOroii/iMK99FEUZbmiKJsURdmoKMqvWuv9KIqSoijKGkVRvg3dy/2t9V7CrmlWFOVrRVHea833oiiKNxTDN0po76NWfC+dFEV5XVEUj6IomqIoZ7XWe4kqXdfb7YvgQofbgAFAEvAtMMygWM4FxgAbwo79DXCGPjuBh0Kfh4ViTQb6h+7BHDq3BjgTUID/AReFjt8EzA99ng68GvrcmeBIys5AZuhzZjPvpQcwJvQ5g+C6isNa4/2Efjc99DkR+DIUT6u7l7B7ugN4CXivlf935gWy6hxrrffyHDAr9DkJ6NRa7yWaL8MDMPTm4SxgSdj3u4G7DYzHyvEJajPQI/S5B7C5vjgJDsc/K1TGE3Z8BvBkeJnQ5wSgMPQf8bEyoXNPAjOifF9vE1z4t1XfD2ABviI4Z69V3gvBSe8fAefxfYJqrffi5cQE1eruBegIbCc0aK0130u0X+39EV/Ea/4ZpJuu6/tCn/cD3UKfT7aG4e56jh9XR9d1H3AE6HKSa0VF6FHC6QRbHq3yfkKPxL4BCoAPdV1vtfcCPALcCQTCjrXWe9GBZYqirFMU5f/bu7cQq+oojuPfn1gpWqQoUQRZFkVqGo4VKWERmGYGlvgQdJPCMCvLlxAGX3rRyCwrKKMLhqVpUg+JGIZWljV4m4JuOoEWGYJmV8xZPfzXafYczljMnDz7P64PbPZ//vucvf9rLqyzL/Nf92Ycy/nAT8BLful1uaQBmcZSVyd7gsqGpY83Wf1PgKSBwBrgITP7ubgtp3jM7JiZjSGdfVwhaWTV9ixikTQVOGBmLV29JpdY3AT/uUwG5ki6prgxo1j6ki7vP2dmlwO/ki7p/SOjWOrqZE9QJ6x0cTf9KOlsAF8f8P6uxr3f29X9nd4jqS/pssLB4+yrRySdQkpOr5nZ2tzjATCzQ8Am4IZMYxkPTJPURiphc52kFZnGgpnt9/UB4C1SmZ4cY9kH7PMzc4A3SQkrx1jqq9HXGBu5kD657CGdYlcekhjRwPEMo/M9qMV0vkm6yNsj6HyTdA9d3ySd4v1z6HyTdJW3B5Oufw/yZS8wuIdxCHgVeLKqP7t4gKHAmd7uD2wBpuYYS1VcE+m4B5VdLMAA4PRC+yPSB4fsYvF9bgEu9vZCjyPLWOq5NHwAjV5I1Xy/Ij0Js6CB41gJ/AAcJX2imkW6Rvwe8DWwsfiLAyzwMX+JP6nj/U1Aq29bRsdsIf2A1aR5D7cBFxTec7f3fwPcVYdYJpAuR+wCdvgyJcd4gMuA7R5LK9Ds/dnFUhXXRDoSVHaxkJ683enL5/jfbo6x+P7GAJ/579k6UrLIMpZ6LjHVUQghhFI62e9BhRBCKKlIUCGEEEopElQIIYRSigQVQgihlCJBhRBCKKVIUCFkRNJCSfMbPY4QToRIUCGEEEopElTImqRhXj/nBaV6TRsk9fdt70tq8vYQn+IHSXdKWuc1dtok3S/pYZ+o82NJg2scZ6ikNZI+9WW89y+V1OztSZI2S+oj6Savu7Nd0kZJZ/lrFkp6RdIWSd9Jmi5pkdfwWe9TRFVqHVX6t0m6sMaYhvt7Wnx/l3j/DEmtSjWsNv8v3/gQToBIUKE3uAh4xsxGAIeAW/7De0YC04FxwGPAb5Ym6twK3F7j9UuBJWY2zve/3PsfBWZKuhZ4ivSf+O3AB8BVvs/XSTOIVwwnlbuYBqwANpnZKOB34MbC6w57/zLSLOTVngfmmtlYYD7wrPc3A5PMbLQfI4Qs9W30AEKog71mtsPbLaQ5Df/NJjM7AhyRdBh4x/t3k6Y3qnY9cKkXKAU4Q9JAM/tF0j3AZmCemX3r288F3vBJPk8lzXFW8a6ZHZW0m1Q0c33h2MWxryyslxQH4zPFXw2sLozpNF9/CLwsaRWwlhAyFQkq9AZ/FtrHSJO6AvxFx1WCfsd5T3vh63Zq/130IZ0R/VFj2yjSzNDnFPqeBp4ws7clTSRNANrp2GbWLumodcw3Vn1s66JdGc8hS+UmOjGz2ZKuJJ2NtUgaa2YHa4w7hFKLS3yhN2sDxnr71h7uawMwt/KFpDG+Pg94hFSUcbInBkjlDCplC+7o5jFnFtZbixss1dfaK2mGj0OSRnt7uJl9YmbNpEJ4xXIKIWQjElTozR4H7pO0HRjSw309ADRJ2iXpC2C20rW1F4H5ZvY9aQb65ZL6kc6YVktqIZXX7o5BknYBDwLzamy/DZglqTKj983ev9gfrmgllaHY2c3jh9BQMZt5CCXkTxw2mVl3k1sI2YszqBBCCKUUZ1AhhBBKKc6gQgghlFIkqBBCCKUUCSqEEEIpRYIKIYRQSpGgQgghlNLfiV+9k0Yi8AMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a21576ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('cnn_data.dill', 'rb') as f:\n",
    "    data = dill.load(f)\n",
    "\n",
    "def save_plots(filename, loss, acc):\n",
    "\n",
    "      examples = np.arange(len(loss)) * 500 * 128\n",
    "      loss = [loss[i] / ((i + 1) * 500) for i in range(len(loss))]\n",
    "\n",
    "      fig, ax1 = plt.subplots()\n",
    "\n",
    "      color = 'tab:red'\n",
    "      ax1.set_xlabel('num examples')\n",
    "      ax1.set_ylabel('loss', color=color)\n",
    "      ax1.plot(examples, loss, color=color)\n",
    "      ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "      ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "      color = 'tab:blue'\n",
    "      ax2.set_ylabel('test acc', color=color)  # we already handled the x-label with ax1\n",
    "      ax2.plot(examples, acc, color=color)\n",
    "      ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "      fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "      plt.savefig(filename=filename)\n",
    "    \n",
    "save_plots('cnn.png', data['train_loss'], data['val_acc'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
